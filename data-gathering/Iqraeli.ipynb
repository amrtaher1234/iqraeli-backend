{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1N2wEodq8APRsZubrpUL2zbgA95W-8Mth",
      "authorship_tag": "ABX9TyN5gv9zVQbnTTgkwTDItVQB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amrtaher1234/iqraeli-backend/blob/main/data-gathering/Iqraeli.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Iqraeli Embedding Generator\n",
        "\n",
        "This notebook generates the embedding of the quran using the quran text per each Ayah and the Ayah name and tafsiir using:\n",
        "\n",
        "- OpenAI for the embedding generation and generating embedding per query\n",
        "- Scikit learn for the embedding comparison and similarity calcualtion\n",
        "- A [quran json](https://drive.google.com/file/d/1yGOGqdnNqXm8ajxsEFU3KG_ckRXlMFsC/view?usp=sharing) that was cleaned and pre-processed to gather the Quran data as well as the Tafsiir for eah ayah, it follows the following schema\n",
        "\n",
        "\n",
        "```js\n",
        "Interface QuranVerse {\n",
        "  juz: number;\n",
        "  juz_name_arabic: string;\n",
        "  juz_name_english: string;\n",
        "  surah_number: number;\n",
        "  surah_name_arabic: string;\n",
        "  surah_name_english: string;\n",
        "  revelation_location: string;\n",
        "  aya_number: number;\n",
        "  english_translation: string;\n",
        "  arabic_diacritics: string;\n",
        "  arabic_clean: string;\n",
        "  arabic_words_count: number;\n",
        "  arabic_letters_count: number;\n",
        "  tafseer: string;\n",
        "  merged_tafseer_text: string;\n",
        "}\n",
        "\n",
        "```\n",
        "\n",
        "## Get Started\n",
        "\n",
        "to get started you need to have the `quran-tafseer.json` somewhere either in your local env or hosted somewhere else (I'm using Drive in my code). The generated embedding object is around 320MB so no need for much storage.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oIpyNO5lANWI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNxE9M7tFIbK",
        "outputId": "72776c19-8ee3-4393-de06-7003e8a2aa60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.6.1-py3-none-any.whl (225 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.4/225.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Collecting typing-extensions<5,>=4.7 (from openai)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: typing-extensions, h11, httpcore, httpx, openai\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 openai-1.6.1 typing-extensions-4.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup and imports\n",
        "\n",
        "from openai import OpenAI\n",
        "from google.colab import drive\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import json\n",
        "from google.colab import userdata\n",
        "\n",
        "client = OpenAI(api_key=userdata.get('OPEN_AI_KEY'))\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "tb-B81CiFonl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4cbabe7-d8ea-4d8d-e2d0-9b4b25f865e6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Utilities\n",
        "def load_json_data(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        return json.load(file)\n",
        "\n",
        "def write_json_data(data, pathname):\n",
        "     with open(pathname, 'w', encoding='utf-8') as file:\n",
        "        json.dump(data, file, ensure_ascii=False, indent=4)\n"
      ],
      "metadata": {
        "id": "8RCdqpGlGWVH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "KYh2cMCOGhML"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_embeddings(objects, pathname):\n",
        "    arabic_texts = [obj['merged_tafseer_text'] for obj in objects]\n",
        "\n",
        "    embeddings_items = client.embeddings.create(model='text-embedding-ada-002', input=arabic_texts).data\n",
        "    embeddings = []\n",
        "\n",
        "    for item in  embeddings_items:\n",
        "        embeddings.append(item.embedding)\n",
        "\n",
        "\n",
        "    for obj, embedding in zip(objects, embeddings):\n",
        "        obj['embedding'] = embedding\n",
        "        # obj['embedding_text'] = surah_only_embedding\n",
        "\n",
        "    with open(pathname, 'w', encoding='utf-8') as file:\n",
        "        json.dump(objects, file, ensure_ascii=False, indent=4)\n",
        "\n"
      ],
      "metadata": {
        "id": "ozUwhB1NHIib"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quran_with_tafseer = load_json_data('./drive/MyDrive/iqraeli/quran-with-tafseer.json')"
      ],
      "metadata": {
        "id": "W6LsphbgHmBs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_embeddings(objects=quran_with_tafseer[0:2000], pathname='./drive/MyDrive/embeddings/tafseer-with-quran-embeddings.json')\n",
        "create_embeddings(objects=quran_with_tafseer[2000:4000], pathname='.drive/MyDrive/embeddings/tafseer-with-quran-embeddings2.json')\n",
        "create_embeddings(objects=quran_with_tafseer[4000:6000], pathname='.drive/MyDrive/embeddings/tafseer-with-quran-embeddings3.json')\n",
        "create_embeddings(objects=quran_with_tafseer[6000:], pathname='.drive/MyDrive/embeddings/tafseer-with-quran-embeddings4.json')"
      ],
      "metadata": {
        "id": "IFSp2igaHrGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# used for preprocessing and combining quran data, use the `quran-embeddings.json` directly\n",
        "\n",
        "file_paths = [\n",
        "    './embeddings/tafseer-with-quran-embeddings.json',\n",
        "    './embeddings/tafseer-with-quran-embeddings-2.json',\n",
        "    './embeddings/tafseer-with-quran-embeddings-3.json',\n",
        "    './embeddings/tafseer-with-quran-embeddings-4.json'\n",
        "]\n",
        "\n",
        "combined_data = []\n",
        "for path in file_paths:\n",
        "    data = load_json_data(path)\n",
        "    combined_data.extend(data)\n",
        "\n",
        "write_json_data(combined_data, './embeddings/quran-embeddings.json')"
      ],
      "metadata": {
        "id": "h5SyLT0xSXpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def find_closest_5(objects: list, query: str):\n",
        "    query_embedding = client.embeddings.create(model='text-embedding-ada-002', input=query).data[0].embedding\n",
        "    top_objects = []\n",
        "\n",
        "    for obj in objects:\n",
        "        similarity = cosine_similarity([query_embedding], [obj['embedding']])[0][0]\n",
        "        top_objects.append((obj, similarity))\n",
        "        top_objects.sort(key=lambda x: x[1], reverse=True)\n",
        "        top_objects = top_objects[:5]\n",
        "\n",
        "    [print(obj[1]) for obj in top_objects]\n",
        "    return [obj[0] for obj in top_objects]\n"
      ],
      "metadata": {
        "id": "_5R9o_YrIwf4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WqKZ7OklALn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_data = load_json_data('./drive/MyDrive/iqraeli/quran-embeddings.json')\n"
      ],
      "metadata": {
        "id": "PA6HLIAbI39u"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_5_data = find_closest_5(embeddings_data, 'وجعلنا من الماء كل شيءٍ حي')\n",
        "\n",
        "for data in top_5_data:\n",
        "    print(data['surah_name_arabic'])\n",
        "    print(data['aya_number'])\n",
        "    print('-----')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWDiREqSJOd2",
        "outputId": "de7d2d85-a18e-4230-f797-ec05fd58bb89"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8469748036734752\n",
            "0.8296564311865964\n",
            "0.8277813558091472\n",
            "0.8258790754696268\n",
            "0.8244292955156522\n",
            "يس\n",
            "34\n",
            "-----\n",
            "ق\n",
            "9\n",
            "-----\n",
            "المرسلات\n",
            "21\n",
            "-----\n",
            "الحاقة\n",
            "12\n",
            "-----\n",
            "النبأ\n",
            "14\n",
            "-----\n"
          ]
        }
      ]
    }
  ]
}